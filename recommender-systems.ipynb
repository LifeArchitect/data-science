{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook seeks to explore various techniques for implementing recommender systems, namely\n\n* Popularity-based - recommend items with high rating \n    * weighted mean item ratings\n    * trending, last-watched\n    \n    \n* Content-based - recommend similar items\n    * Cosine similarity of item metadata \n    \n    \n* Collaborative Filtering - recommend items that similar users also like\n    * Matrix Factorization\n    * Nearest Neighbours\n    * Deep learning approaches\n    \n\nOther Challenges\n* Cold-Start Problem\n* Efficiency vs Accuracy"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# To create plots\nimport matplotlib.pyplot as plt\n\n# To create interactive plots\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# To shift lists\nfrom collections import deque\n\n# To compute similarities between vectors\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# To use recommender systems\nimport surprise as sp\nfrom surprise.model_selection import cross_validate\n\n# To create deep learning models\nfrom keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\nfrom keras.models import Model\n\n# To create sparse matrices\nfrom scipy.sparse import coo_matrix\n\n# To light fm\nfrom lightfm import LightFM\nfrom lightfm.evaluation import precision_at_k\n\n# To stack sparse matrices\nfrom scipy.sparse import vstack\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Dataset Preprocessing\n\nLets start by using the Netflix prize datasets\n* 17K+ Movies\n* Descriptions of each movie\n* 24M movie ratings from users"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data for all movies\nmovie_titles = pd.read_csv('../input/netflix-prize-data/movie_titles.csv', \n                           encoding = 'ISO-8859-1', \n                           header = None, \n                           names = ['Id', 'Year', 'Name']).set_index('Id')\n\nprint('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\nmovie_titles.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load a movie metadata dataset\nmovie_metadata = pd.read_csv('../input/the-movies-dataset/movies_metadata.csv', low_memory=False)[['original_title', 'overview', 'vote_count']].set_index('original_title').dropna()\n# Remove the long tail of rarly rated moves\nmovie_metadata = movie_metadata[movie_metadata['vote_count']>10].drop('vote_count', axis=1)\n\nprint('Shape Movie-Metadata:\\t{}'.format(movie_metadata.shape))\nmovie_metadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load single data-file\ndf_raw = pd.read_csv('../input/netflix-prize-data/combined_data_1.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n\n# Find empty rows to slice dataframe for each movie\ntmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\nmovie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n\n# Shift the movie_indices by one to get start and endpoints of all movies\nshifted_movie_indices = deque(movie_indices)\nshifted_movie_indices.rotate(-1)\nuser_data = []\n\n# Iterate over all movies \nfor [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n    if df_id_1<df_id_2: # Check if it is the last movie in the file\n        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n    else:\n        tmp_df = df_raw.loc[df_id_1+1:].copy()\n        \n    # Create movie_id column and append df\n    tmp_df['Movie'] = movie_id\n    user_data.append(tmp_df)\n\n# Combine all dataframes\ndf = pd.concat(user_data)\ndel user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\nprint('Shape User-Ratings:\\t{}'.format(df.shape))\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For effecient performance reasons, we want to only get the top 500 movies for this demonstration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter sparse movies\nmin_movie_ratings = 10000\nfilter_movies = (df['Movie'].value_counts()>min_movie_ratings)\nfilter_movies = filter_movies[filter_movies].index.tolist()\n\n# Filter sparse users\nmin_user_ratings = 200\nfilter_users = (df['User'].value_counts()>min_user_ratings)\nfilter_users = filter_users[filter_users].index.tolist()\n\n# Filter all users and movies with low rating count (not useful to us) ~ 4M ratings\ndf_filterd = df[(df['Movie'].isin(filter_movies)) & (df['User'].isin(filter_users))]\ndel filter_movies, filter_users, min_movie_ratings, min_user_ratings\nprint('Shape User-Ratings unfiltered:\\t{}'.format(df.shape))\nprint('Shape User-Ratings filtered:\\t{}'.format(df_filterd.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle df and Split into training and testing data \ndf_filterd = df_filterd.drop('Date', axis=1).sample(frac=1).reset_index(drop=True)\ntest_size = 100000\ndf_train = df_filterd[:-test_size]\ndf_test = df_filterd[-test_size:]\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we want to create a large, sparse matrix to facilitate the recommendation algorithms that we will be building, which consists of 20M users by 490+ movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a user-movie matrix with empty values\ndf_p = df_train.pivot_table(index='User', columns='Movie', values='Rating')\nprint('Shape User-Movie-Matrix:\\t{}'.format(df_p.shape))\ndf_p.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Popularity-Based Recommendation\n\nComputing the mean rating for all movies creates a ranking. The recommendation will be the same for all users and can be used if there is no information on the user. Variations of this approach can be separate rankings for each country/year/gender/... and to use them individually to recommend movies/items to the user. However,using the rating of a movie alone is biased and favours movies with fewer ratings, since large numbers of ratings tend to be less extreme in its mean ratings. To tackle the problem of the unstable mean with few ratings e.g. IDMb uses a weighted rating. Many good ratings outweigh few in this algorithm.\n\nQuestions\n* why dont we use the original count and rating? because we only want to include active users\n* how do we deal with NaN values? we dont, we just count those without NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of minimum votes to be considered\nm = 1000\nn = 10\nC = df_p.stack().mean() # Mean rating for all movies\nR = df_p.mean(axis=0).values # Mean rating for all movies separately\nv = df_p.count().values # Rating count for all movies separately\nweighted_score = (v/ (v+m) *R) + (m/ (v+m) *C)\nweighted_ranking = np.argsort(weighted_score)[::-1]\nweighted_score = np.sort(weighted_score)[::-1]\nweighted_movie_ids = df_p.columns[weighted_ranking]\nratings_count = df_p.count(axis=0).rename('Rating-Count').to_frame()\n\n# Join labels and predictions\ndf_prediction = df_test.set_index('Movie').join(pd.DataFrame(weighted_score, index=weighted_movie_ids, columns=['Prediction']))[['Rating', 'Prediction']]\ny_true = df_prediction['Rating']\ny_pred = df_prediction['Prediction']\nrmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n\n# Create DataFrame for plotting\ndf_plot = pd.DataFrame(weighted_score[:n], columns=['Rating'])\ndf_plot.index = weighted_movie_ids[:10]\nranking_weighted_rating = df_plot.join(ratings_count).join(movie_titles)\n\n\n# Create trace\ntrace = go.Bar(x = ranking_weighted_rating['Rating'],\n               text = ranking_weighted_rating['Name'].astype(str) +': '+ ranking_weighted_rating['Rating-Count'].astype(str) + ' Ratings',\n               textposition = 'outside',\n               textfont = dict(color = '#000000'),\n               orientation = 'h',\n               y = list(range(1, n+1)),\n               marker = dict(color = '#db0000'))\n# Create layout\nlayout = dict(title = 'Ranking Of Top {} Weighted-Movie-Ratings: {:.4f} RMSE'.format(n, rmse),\n              xaxis = dict(title = 'Weighted Rating',\n                          range = (4.15, 4.6)),\n              yaxis = dict(title = 'Movie'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ranking_weighted_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#movie_metadata = pd.read_csv('../input/the-movies-dataset/movies_metadata.csv', low_memory=False)\n#movie_metadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_plot, weighted_ranking, weighted_score, weighted_movie_ids, ratings_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Content-Based: User Similarity\n\n\"Other users are also watching\"\n\nThis recommendation strategy recommends movies that other similar users are also interested in. \n\nInterpreting each row of the matrix as a vector, a similarity between all user-vectors can be computed. This enables us to find all similar users and to work on user-specific recommendations. Recommending high rated movies of similar users to a specific user seems reasonable.\n\nSince there are still empty values left in the matrix, we have to use a reliable way to impute a decent value. A simple first approach is to fill in the mean of each user into the empty values.\n\nAfterwards the ratings of all similar users will be weighted with their similarity score and the mean will be computed. Filtering for the unrated movies of a user reveals the best recommendations.\n\nYou can easily adapt this process to find similar items by computing the item-item similarity the same way. Since the matrix is mostly sparse and there are more users than items, this could be better for the RMSE score."},{"metadata":{"trusted":true},"cell_type":"code","source":"user_index = 0\nn_recommendation = 100\nn_plot = 10\ndf_p_imputed = df_p.T.fillna(df_p.mean(axis=1)).T # Fill in missing values with mean user ratings\ndf_p_imputed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute similarity between all users and remove self-similarity\nsimilarity = cosine_similarity(df_p_imputed.values)\nsimilarity -= np.eye(similarity.shape[0])\nprint(np.shape(similarity))\n# similarity # An NxN matrix of similarity score for each user","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_movies_of_top_n_similar_users(similarity, user_index=0, n_recommendation=100):\n    # Sort similar users by index and score\n    similar_user_index = np.argsort(similarity[user_index])[::-1]\n    similar_user_score = np.sort(similarity[user_index])[::-1]\n\n    # Get movies that user has not rated / watched\n    unrated_movies = df_p.iloc[user_index][df_p.iloc[user_index].isna()].index\n\n    # Weight ratings of the top n most similar users with their rating and compute the mean for each movie\n    mean_movie_recommendations = (df_p_imputed.iloc[similar_user_index[:n_recommendation]].T * similar_user_score[:n_recommendation]).T.mean(axis=0)\n\n    # Filter for unrated movies and sort results\n    best_movie_recommendations = mean_movie_recommendations[unrated_movies].sort_values(ascending=False).to_frame().join(movie_titles)\n\n    return best_movie_recommendations\n\nget_movies_of_top_n_similar_users(similarity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clear up ram\ndel similarity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Content-Based: TFIDF Movie Metadata Similarity\n\n\"Because you watched this\" \n\nIf there is no historical data for a user or there is reliable metadata for each movie, it can be useful to compare the metadata of the movies to find similar ones.\nIn this approch I will use the movie description to create a TFIDF-matrix, which counts and weights words in all descriptions, and compute a cosine similarity between all of those sparse text-vectors. This can easily be extended to more or different features if you like.\nUnfortunately it is impossible for this model to compute a RMSE score, since the model does not recommend the movies directly.\nIn this way it is possible to find movies closly related to each other, but it is hard to find movies of different genres/categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_similar_movies(movie_metadata, n=10, movie='Batman Begins'):\n    # Create tf-idf matrix for text comparison and compute cosine similarity between all movies\n    tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n    tfidf_matrix = tfidf.fit_transform(movie_metadata['overview'].dropna())\n    similarity = cosine_similarity(tfidf_matrix)\n    similarity -= np.eye(similarity.shape[0])\n\n    # Get index of movie and get titles of similar movies\n    index = movie_metadata.reset_index(drop=True)[movie_metadata.index==movie].index[0]\n    similar_movies_index = np.argsort(similarity[index])[::-1][:n]\n    similar_movies_score = np.sort(similarity[index])[::-1][:n]\n    similar_movie_titles = movie_metadata.iloc[similar_movies_index].index\n    del similarity, tfidf_matrix\n    \n    return similar_movie_titles\n\nget_top_n_similar_movies(movie_metadata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Collaborative Filtering: Matrix Factorization with Gradient Descent\n\nThe user-movie rating matrix is high dimensional and sparse, therefore I am going to reduce the dimensionality to represent the data in a dense form.\nUsing matrix factorisation a large matrix can be estimated/decomposed into two long but slim matrices. With gradient descent it is possible to adjust these matrices to represent the given ratings. The gradient descent algorithm finds latent variables which represent the underlying structure of the dataset. Afterwards these latent variables can be used to reconstruct the original matrix and to predict the missing ratings for each user.\nIn this case the model has not been trained to convergence and is not hyperparameter optimized."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create user- & movie-id mapping\nuser_id_mapping = {id:i for i, id in enumerate(df_filterd['User'].unique())}\nmovie_id_mapping = {id:i for i, id in enumerate(df_filterd['Movie'].unique())}\n\n\n# Create correctly mapped train- & testset\ntrain_user_data = df_train['User'].map(user_id_mapping)\ntrain_movie_data = df_train['Movie'].map(movie_id_mapping)\n\ntest_user_data = df_test['User'].map(user_id_mapping)\ntest_movie_data = df_test['Movie'].map(movie_id_mapping)\n\n\n# Get input variable-sizes\nusers = len(user_id_mapping)\nmovies = len(movie_id_mapping)\nembedding_size = 10\n\n\n##### Create model\n# Set input layers\nuser_id_input = Input(shape=[1], name='user')\nmovie_id_input = Input(shape=[1], name='movie')\n\n# Create embedding layers for users and movies\nuser_embedding = Embedding(output_dim=embedding_size, \n                           input_dim=users,\n                           input_length=1, \n                           name='user_embedding')(user_id_input)\nmovie_embedding = Embedding(output_dim=embedding_size, \n                            input_dim=movies,\n                            input_length=1, \n                            name='item_embedding')(movie_id_input)\n\n# Reshape the embedding layers\nuser_vector = Reshape([embedding_size])(user_embedding)\nmovie_vector = Reshape([embedding_size])(movie_embedding)\n\n# Compute dot-product of reshaped embedding layers as prediction\ny = Dot(1, normalize=False)([user_vector, movie_vector])\n\n# Setup model\nmodel = Model(inputs=[user_id_input, movie_id_input], outputs=y)\nmodel.compile(loss='mse', optimizer='adam')\n\n\n# Fit model\nmodel.fit([train_user_data, train_movie_data],\n          df_train['Rating'],\n          batch_size=256, \n          epochs=1,\n          validation_split=0.1,\n          shuffle=True)\n\n# Test model\ny_pred = model.predict([test_user_data, test_movie_data])\ny_true = df_test['Rating'].values\n\n#  Compute RMSE\nrmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\nprint('\\n\\nTesting Result With Keras Matrix-Factorization: {:.4f} RMSE'.format(rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matrix Factorization: Deep Learning using various MetaData\n\nWith its embedding layers this is similar to the matrix factorization approach above, but instead of using a fixed dot-product as recommendation we will utilize some dense layers so the network can find better combinations. One advantage of deep learning models is, that movie-metadata can easily be added to the model.\n\nI will tf-idf transform the short description of all movies to a sparse vector. The model will learn to reduce the dimensionality of this vector and how to combine metadata with the embedding of the user-id and the movie-id. In this way you can add any additional metadata to your own recommender.\n\nThese kind of hybrid systems can learn how to reduce the impact of the cold start problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"user_id_mapping = {id: i for i, id in enumerate(df['User'].unique())}\nmovie_id_mapping = {id: i for i, id in enumerate(df['Movie'].unique())}\ndf['User'] = df['User'].map(user_id_mapping)\ndf['Movie'] = df['Movie'].map(movie_id_mapping)\n\n# Preprocess metadata\ntmp_metadata = movie_metadata.copy()\ntmp_metadata.index = tmp_metadata.index.str.lower()\n\n# Preprocess titles\ntmp_titles = movie_titles.drop('Year', axis=1).copy()\ntmp_titles = tmp_titles.reset_index().set_index('Name')\ntmp_titles.index = tmp_titles.index.str.lower()\n\n# Combine titles and metadata\ndf_id_descriptions = tmp_titles.join(tmp_metadata).dropna().set_index('Id')\ndf_id_descriptions['overview'] = df_id_descriptions['overview'].str.lower()\ndel tmp_metadata,tmp_titles\ndf_id_descriptions.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter all ratings with metadata, split into training and testing sets\ndf_hybrid = df.drop('Date', axis=1).set_index('Movie').join(df_id_descriptions).dropna().drop('overview', axis=1).reset_index().rename({'index':'Movie'}, axis=1)\nn = 100000\ndf_hybrid = df_hybrid.sample(frac=1).reset_index(drop=True)\ndf_hybrid_train = df_hybrid[:1500000]\ndf_hybrid_test = df_hybrid[-n:]\n\n\n# Create tf-idf matrix for text comparison\ntfidf = TfidfVectorizer(stop_words='english')\ntfidf_hybrid = tfidf.fit_transform(df_id_descriptions['overview'])\nmapping = {id:i for i, id in enumerate(df_id_descriptions.index)}\n\ntrain_tfidf = []\n# Iterate over all movie-ids and save the tfidf-vector\nfor id in df_hybrid_train['Movie'].values:\n    index = mapping[id]\n    train_tfidf.append(tfidf_hybrid[index])\n    \ntest_tfidf = []\n# Iterate over all movie-ids and save the tfidf-vector\nfor id in df_hybrid_test['Movie'].values:\n    index = mapping[id]\n    test_tfidf.append(tfidf_hybrid[index])\n\n\n# Stack the sparse matrices\ntrain_tfidf = vstack(train_tfidf)\ntest_tfidf = vstack(test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Setup the network variables\nuser_embed = 10\nmovie_embed = 10\n\nuser_id_input = Input(shape=[1], name='user') # Create two input layers\nmovie_id_input = Input(shape=[1], name='movie')\ntfidf_input = Input(shape=[24144], name='tfidf', sparse=True)\n\n# Create separate embeddings for users and movies\nuser_embedding = Embedding(output_dim=user_embed,\n                           input_dim=len(user_id_mapping),\n                           input_length=1,\n                           name='user_embedding')(user_id_input)\nmovie_embedding = Embedding(output_dim=movie_embed,\n                            input_dim=len(movie_id_mapping),\n                            input_length=1,\n                            name='movie_embedding')(movie_id_input)\n\n# Create 2 layers, reshape and concatenate them\ntfidf_vectors = Dense(128, activation='relu')(tfidf_input)\ntfidf_vectors = Dense(32, activation='relu')(tfidf_vectors)\nuser_vectors = Reshape([user_embed])(user_embedding)\nmovie_vectors = Reshape([movie_embed])(movie_embedding)\nboth = Concatenate()([user_vectors, movie_vectors, tfidf_vectors])\ndense = Dense(512, activation='relu')(both)\ndense = Dropout(0.2)(dense)\noutput = Dense(1)(dense)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and compile model\nmodel = Model(inputs=[user_id_input, movie_id_input, tfidf_input], outputs=output)\nmodel.compile(loss='mse', optimizer='adam')\n\n\n# Train and test the network\nmodel.fit([df_hybrid_train['User'], df_hybrid_train['Movie'], train_tfidf],\n          df_hybrid_train['Rating'],\n          batch_size=1024, \n          epochs=2,\n          validation_split=0.1,\n          shuffle=True)\n\ny_pred = model.predict([df_hybrid_test['User'], df_hybrid_test['Movie'], test_tfidf])\ny_true = df_hybrid_test['Rating'].values\n\nrmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\nprint('\\n\\nTesting Result With Keras Hybrid Deep Learning: {:.4f} RMSE'.format(rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Surprise Library\n\nThe surprise library was built for creating and analyzing recommender systems.\nIt has to be mentioned that most of the built-in algorithms use some kind of the above approches. I am going to compare these algorithms to each other in this section using 5-fold crossvalidation. Since the algorithms and the dataset have a large memoryfootprint the comparison will be executed on a subsampled dataset which is not comparable to the above models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run 5-fold cross-validation on the famous SVD algorithm \nmovies = sp.Dataset.load_from_df(df_filterd[['User', 'Movie', 'Rating']].sample(50000), sp.Reader())\nmovies","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First lets test out the famous [SVD algorithm](http://https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD ) by Simon Funk, used in the Netflix Prize Competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validate(algo=sp.SVD(), data=movies, measures=['RMSE', 'MAE'], cv=5, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import accuracy\nfrom surprise import Dataset\nfrom surprise.model_selection import train_test_split, KFold\nfrom collections import defaultdict\n\ndef get_top_n(predictions, n=10):\n    # Map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # sort predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n\n# Train SVD algorithm on movielens dataset.\nmovies = Dataset.load_builtin('ml-100k')\ntrain_data, test_data = train_test_split(movies, test_size=0.2)\nmodel = sp.SVD()\npredictions = model.fit(train_data).test(test_data)\naccuracy.rmse(predictions)\nmodel.predict(uid=str(196), iid=str(302)) # predict a rating that user(i) would give item(j)\ntop_n = get_top_n(predictions, n=10)\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, _) in user_ratings])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision@K and Recall@K"},{"metadata":{"trusted":true},"cell_type":"code","source":"def precision_recall_at_k(predictions, k=10, threshold=3.5):\n    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n\n    precisions, recalls = {}, {} # Map the predictions to each user.\n    user_est_true = defaultdict(list)\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n\n    for uid, user_ratings in user_est_true.items():\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Count number of relevant items and recommended items in top k\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n\n        # Recall@K: Proportion of relevant items that are recommended\n        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n\n    return precisions, recalls\n\nfor trainset, testset in KFold(n_splits=5).split(movies):\n    model.fit(trainset)\n    predictions = model.test(testset)\n    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n\n    # Precision and recall can then be averaged over all users\n    print(sum(prec for prec in precisions.values()) / len(precisions))\n    print(sum(rec for rec in recalls.values()) / len(recalls))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, lets try a [K Nearest Neighbours](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline) basic collaborative filtering algorithm taking into account a baseline rating."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run 5 fold validation on KNN approach using baseline\ncross_validate(algo=sp.KNNBaseline(), data=movies, measures=['RMSE', 'MAE'], cv=5, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import io \n\ndef read_item_names():\n    \"\"\"Read the u.item file from MovieLens 100-k dataset and return two\n    mappings to convert raw ids into movie names and movie names into raw ids.\n    \"\"\"\n    file_name = sp.get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n    rid_to_name = {}\n    name_to_rid = {}\n    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n        for line in f:\n            line = line.split('|')\n            rid_to_name[line[0]] = line[1]\n            name_to_rid[line[1]] = line[0]\n\n    return rid_to_name, name_to_rid\n\n# Train the KNN algortihm to compute the similarities between items\ndata = Dataset.load_builtin('ml-100k')\ntrainset = data.build_full_trainset()\nsim_options = {'name': 'pearson_baseline', 'user_based': False}\nalgo = sp.KNNBaseline(sim_options=sim_options)\nalgo.fit(trainset)\n\n# Read the mappings raw id <-> movie name\nrid_to_name, name_to_rid = read_item_names()\ntoy_story_raw_id = name_to_rid['Toy Story (1995)']\ntoy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)\n\n# Retrieve inner ids of the nearest neighbors of Toy Story.\ntoy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)\n\n# Convert inner ids of the neighbors into names.\ntoy_story_neighbors = (algo.trainset.to_raw_iid(inner_id) for inner_id in toy_story_neighbors)\ntoy_story_neighbors = (rid_to_name[rid] for rid in toy_story_neighbors)\n\nprint()\nprint('The 10 nearest neighbors of Toy Story are:')\nfor movie in toy_story_neighbors:\n    print(movie)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}